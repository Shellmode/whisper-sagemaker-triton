FROM nvcr.io/nvidia/tritonserver:24.05-py3
LABEL maintainer="NVIDIA"
LABEL repository="tritonserver"

RUN apt update && apt-get install -y ffmpeg


WORKDIR /workspace
COPY requirements.txt /workspace/requirements.txt
COPY serve /workspace/serve
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com tensorrt-llm==0.11.0.dev2024052800 && \
pip install mpmath==1.3.0 tritonclient[all] && \
pip install -r requirements.txt && \
chmod +x /workspace/serve

# RUN pip install -U "huggingface_hub[cli]"pip install -U "huggingface_hub[cli]"

# RUN huggingface-cli download --local-dir ./whisper_large_v3_trtllm_triton yuekai/whisper_large_v3_trtllm_triton
# RUN rm -r ./whisper_large_v3_trtllm_triton/.huggingface

# RUN git clone https://github.com/NVIDIA/TensorRT-LLM.git;

# 让端口8080在容器外可用
# EXPOSE 8001

# 定义环境变量
ENV PATH="/workspace:${PATH}"

# 运行serve
ENTRYPOINT []
CMD ["serve"]